{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Method to draw boundary around the detected feature\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    # Converting image to gray-scale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        coords = [x, y, w, h]\n",
    "    return coords\n",
    "\n",
    "\n",
    "# Method to detect the features\n",
    "def detect(img, faceCascade, eyeCascade, noseCascade, mouthCascade):\n",
    "    color = {\"red\":(255,0,0), \"blue\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color['blue'], \"Face\")\n",
    "    # If feature is detected, the draw_boundary method will return the x,y coordinates and width and height of rectangle else the length of coords will be 0\n",
    "    if len(coords)==4:\n",
    "        # Updating region of interest by cropping image\n",
    "        roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "        # Passing roi, classifier, scaling factor, Minimum neighbours, color, label text\n",
    "        coords = draw_boundary(roi_img, eyeCascade, 1.1, 14, color['red'], \"Eye\")\n",
    "        coords = draw_boundary(roi_img, noseCascade, 1.1, 5, color['green'], \"Nose\")\n",
    "        coords = draw_boundary(roi_img, mouthCascade, 1.1, 20, color['white'], \"Mouth\")\n",
    "    return img\n",
    "\n",
    "\n",
    "# Loading classifiers\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eyesCascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "noseCascade = cv2.CascadeClassifier('Nariz.xml')\n",
    "mouthCascade = cv2.CascadeClassifier('Mouth.xml')\n",
    "\n",
    "# Capturing real time video stream. 0 for built-in web-cams, 0 or -1 for external web-cams\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Reading image from video stream\n",
    "    _, img = video_capture.read()\n",
    "    # Call method we defined above\n",
    "    img = detect(img, faceCascade, eyesCascade, noseCascade, mouthCascade)\n",
    "    # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# releasing web-cam\n",
    "video_capture.release()\n",
    "# Destroying output window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected  0  images\n",
      "Collected  50  images\n",
      "Collected  100  images\n",
      "Collected  150  images\n",
      "Collected  200  images\n",
      "Collected  250  images\n",
      "Collected  300  images\n",
      "Collected  350  images\n",
      "Collected  400  images\n",
      "Collected  450  images\n",
      "Collected  500  images\n",
      "Collected  550  images\n",
      "Collected  600  images\n",
      "Collected  650  images\n",
      "Collected  700  images\n",
      "Collected  750  images\n",
      "Collected  800  images\n",
      "Collected  850  images\n",
      "Collected  900  images\n",
      "Collected  950  images\n",
      "Collected  1000  images\n",
      "Collected  1050  images\n",
      "Collected  1100  images\n",
      "Collected  1150  images\n",
      "Collected  1200  images\n",
      "Collected  1250  images\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Method to generate dataset to recognize a person\n",
    "def generate_dataset(img, id, img_id):\n",
    "    # write image in data dir\n",
    "    cv2.imwrite(\"data/user.\"+str(id)+\".\"+str(img_id)+\".jpg\", img)\n",
    "\n",
    "# Method to draw boundary around the detected feature\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "    # Converting image to gray-scale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        coords = [x, y, w, h]\n",
    "    return coords\n",
    "\n",
    "# Method to detect the features\n",
    "def detect(img, faceCascade, img_id):\n",
    "    color = {\"red\":(255,0,0), \"blue\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color['blue'], \"Face\")\n",
    "    # If feature is detected, the draw_boundary method will return the x,y coordinates and width and height of rectangle else the length of coords will be 0\n",
    "    if len(coords)==4:\n",
    "        # Updating region of interest by cropping image\n",
    "        roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "        # Assign unique id to each user\n",
    "        user_id = 1\n",
    "        # img_id to make the name of each image unique\n",
    "        generate_dataset(roi_img, user_id, img_id)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Loading classifiers\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Capturing real time video stream. 0 for built-in web-cams, 0 or -1 for external web-cams\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize img_id with 0\n",
    "img_id = 0\n",
    "\n",
    "while True:\n",
    "    if img_id % 50 == 0:\n",
    "        print(\"Collected \", img_id,\" images\")\n",
    "    # Reading image from video stream\n",
    "    _, img = video_capture.read()\n",
    "    # Call method we defined above\n",
    "    img = detect(img, faceCascade, img_id)\n",
    "    # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    img_id += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# releasing web-cam\n",
    "video_capture.release()\n",
    "# Destroying output window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import  Image\n",
    "import os, cv2\n",
    "\n",
    "# Method to train custom classifier to recognize face\n",
    "def train_classifer(data_dir):\n",
    "    # Read all the images in custom data-set\n",
    "    path = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "    faces = []\n",
    "    ids = []\n",
    "\n",
    "    # Store images in a numpy format and ids of the user on the same index in imageNp and id lists\n",
    "    for image in path:\n",
    "        img = Image.open(image).convert('L')\n",
    "        imageNp = np.array(img, 'uint8')\n",
    "        id = int(os.path.split(image)[1].split(\".\")[1])\n",
    "\n",
    "        faces.append(imageNp)\n",
    "        ids.append(id)\n",
    "\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    # Train and save classifier\n",
    "    clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "    clf.train(faces, ids)\n",
    "    clf.write(\"classifier.yml\")\n",
    "\n",
    "\n",
    "train_classifer(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "    # Converting image to gray-scale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # detecting features in gray-scale image, returns coordinates, width and height of features\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    coords = []\n",
    "    # drawing rectangle around the feature and labeling it\n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        # Predicting the id of the user\n",
    "        id, _ = clf.predict(gray_img[y:y+h, x:x+w])\n",
    "        # Check for id of user and label the rectangle accordingly\n",
    "        if id==2:\n",
    "            cv2.putText(img, \"Navarun\", (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        coords = [x, y, w, h]\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Method to recognize the person\n",
    "def recognize(img, clf, faceCascade):\n",
    "    color = {\"blue\": (255, 0, 0), \"red\": (0, 0, 255), \"green\": (0, 255, 0), \"white\": (255, 255, 255)}\n",
    "    coords = draw_boundary(img, faceCascade, 1.1, 10, color[\"white\"], \"Face\", clf)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Loading classifier\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loading custom classifier to recognize\n",
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.read(\"classifier.xml\")\n",
    "\n",
    "# Capturing real time video stream. 0 for built-in web-cams, 0 or -1 for external web-cams\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Reading image from video stream\n",
    "    _, img = video_capture.read()\n",
    "    # Call method we defined above\n",
    "    img = recognize(img, clf, faceCascade)\n",
    "    # Writing processed image in a new window\n",
    "    cv2.imshow(\"face detection\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# releasing web-cam\n",
    "video_capture.release()\n",
    "# Destroying output window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
